import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd

st.set_page_config(page_title="Forex Analiz Pro", layout="wide")

def get_signal(title, desc=""):
    """BaÅŸlÄ±q vÉ™ xÃ¼lasÉ™yÉ™ É™sasÉ™n siqnal tÉ™yin edir"""
    text = (title + " " + desc).lower()
    buy_words = ['bullish', 'artÄ±ÅŸ', 'yÃ¼kseliÅŸ', 'long', 'destek', 'alÄ±m', 'al']
    sell_words = ['bearish', 'dÃ¼ÅŸÃ¼ÅŸ', 'short', 'direnÃ§', 'satÄ±ÅŸ', 'sat']
    
    if any(word in text for word in buy_words):
        return "ğŸŸ¢ LONG (AlÄ±ÅŸ Meyilli)"
    elif any(word in text for word in sell_words):
        return "ğŸ”´ SHORT (SatÄ±ÅŸ Meyilli)"
    return "ğŸŸ¡ NEYTRAL / GÃ¶zlÉ™"

def get_dailyforex():
    url = "https://www.dailyforex.com/forex-technical-analysis/page-1"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    try:
        # DailyForex bÉ™zÉ™n cookies tÉ™lÉ™b edir, session istifadÉ™ edÉ™k
        session = requests.Session()
        response = session.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        analizler = []
        
        # Selector-u daha Ã¼mumi tutaq
        items = soup.select('div.daily-analysis-item') or soup.select('article')
        
        for item in items[:10]:
            title_el = item.find('h2') or item.find('h3')
            link_el = item.find('a')
            if title_el and link_el:
                title = title_el.text.strip()
                link = "https://www.dailyforex.com" + link_el['href'] if not link_el['href'].startswith('http') else link_el['href']
                analizler.append({
                    "MÉ™nbÉ™": "DailyForex",
                    "Analiz": title,
                    "Siqnal": get_signal(title),
                    "Link": link
                })
        return analizler
    except:
        return []

def get_fxstreet():
    url = "https://www.fxstreet.com.tr/analysis/latest"
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        analizler = []
        items = soup.select('h4.fxs_headline_tiny') or soup.find_all('h4')
        
        for item in items[:10]:
            link_el = item.find('a')
            if link_el:
                title = link_el.text.strip()
                analizler.append({
                    "MÉ™nbÉ™": "FXStreet TR",
                    "Analiz": title,
                    "Siqnal": get_signal(title),
                    "Link": link_el['href']
                })
        return analizler
    except:
        return []

st.title("ğŸ“Š Forex Analiz vÉ™ Siqnallar")

if st.sidebar.button('MÉ™lumatlarÄ± YenilÉ™'):
    with st.spinner('MÉ™lumatlar hÉ™r iki saytdan Ã§É™kilir...'):
        data = get_dailyforex() + get_fxstreet()
        
        if data:
            df = pd.DataFrame(data)
            # CÉ™dvÉ™l gÃ¶rÃ¼nÃ¼ÅŸÃ¼
            st.dataframe(df[['MÉ™nbÉ™', 'Analiz', 'Siqnal']], use_container_width=True)
            
            # DetallÄ± xÃ¼lasÉ™ hissÉ™si
            st.subheader("ğŸ“ Analiz DetallarÄ±")
            for item in data:
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.markdown(f"**{item['MÉ™nbÉ™']}**: {item['Analiz']}")
                with col2:
                    st.info(item['Siqnal'])
                st.write(f"[MÉ™qalÉ™yÉ™ keÃ§id]({item['Link']})")
                st.divider()
        else:
            st.error("XÉ™ta: Saytlara qoÅŸulmaq mÃ¼mkÃ¼n olmadÄ±. ZÉ™hmÉ™t olmasa bir az sonra yenidÉ™n yoxlayÄ±n.")
else:
    st.info("Sol paneldÉ™ki 'YenilÉ™' dÃ¼ymÉ™sinÉ™ basaraq É™n son 20 analizi (10+10) gÃ¶rÉ™ bilÉ™rsiniz.")
    
